{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "76f0616d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Eager execution: True\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import tensorflow as tf\n",
    "from models import create_model\n",
    "from data_loader import load_dataset\n",
    "import datetime\n",
    "\n",
    "tf.config.run_functions_eagerly(True)\n",
    "print(\"Eager execution:\", tf.executing_eagerly())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d78afa5f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/Users/rahulshelke/Documents/Data-Science/Hands-on DL/optimizer_model_comparison'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "db2f508f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "README.md                     requirements.txt\n",
      "\u001b[34m__pycache__\u001b[m\u001b[m/                  \u001b[34mresults\u001b[m\u001b[m/\n",
      "data_loader.py                run_experiments.py\n",
      "models.py                     train.py\n",
      "optimizers_comparision.ipynb\n"
     ]
    }
   ],
   "source": [
    "%ls"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ccd747a5",
   "metadata": {},
   "source": [
    "## Load Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "631b11ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_dataset(name):\n",
    "    if name == 'mnist':\n",
    "        data = tf.keras.datasets.mnist\n",
    "    elif name == 'fashion_mnist':\n",
    "        data = tf.keras.datasets.fashion_mnist\n",
    "    elif name == 'cifar10':\n",
    "        data = tf.keras.datasets.cifar10\n",
    "    else:\n",
    "        raise ValueError(\"Unsupported dataset\")\n",
    "\n",
    "    (x_train, y_train), (x_test, y_test) = data.load_data()\n",
    "\n",
    "    if name == 'cifar10':\n",
    "        x_train = x_train.astype('float32') / 255.0\n",
    "        x_test = x_test.astype('float32') / 255.0\n",
    "    else:\n",
    "        x_train = x_train.reshape(-1, 28, 28, 1).astype('float32') / 255.0\n",
    "        x_test = x_test.reshape(-1, 28, 28, 1).astype('float32') / 255.0\n",
    "\n",
    "    return (x_train, y_train), (x_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8da7fdaa",
   "metadata": {},
   "source": [
    "## Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "61280fc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_model(size='small', input_shape=(28, 28, 1), num_classes=10):\n",
    "    model = tf.keras.Sequential()\n",
    "    model.add(tf.keras.Input(shape=input_shape))\n",
    "\n",
    "    if size == 'small':\n",
    "        model.add(tf.keras.layers.Flatten())\n",
    "        model.add(tf.keras.layers.Dense(64, activation='relu'))\n",
    "\n",
    "    elif size == 'medium':\n",
    "        model.add(tf.keras.layers.Conv2D(32, (3, 3), activation='relu'))\n",
    "        model.add(tf.keras.layers.MaxPooling2D((2, 2)))\n",
    "        model.add(tf.keras.layers.Flatten())\n",
    "        model.add(tf.keras.layers.Dense(128, activation='relu'))\n",
    "\n",
    "    elif size == 'large':\n",
    "        model.add(tf.keras.layers.Conv2D(32, (3, 3), activation='relu'))\n",
    "        model.add(tf.keras.layers.Conv2D(64, (3, 3), activation='relu'))\n",
    "        model.add(tf.keras.layers.MaxPooling2D((2, 2)))\n",
    "        model.add(tf.keras.layers.Flatten())\n",
    "        model.add(tf.keras.layers.Dense(256, activation='relu'))\n",
    "        model.add(tf.keras.layers.Dropout(0.5))\n",
    "\n",
    "    model.add(tf.keras.layers.Dense(num_classes, activation='softmax'))\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a95beb9a",
   "metadata": {},
   "source": [
    "## Training Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0fd3cd15",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model_size, dataset_name, optimizer, epochs=5, batch_size=64):\n",
    "    (x_train, y_train), (x_test, y_test) = load_dataset(dataset_name)\n",
    "    input_shape = x_train.shape[1:]\n",
    "    num_classes = 10\n",
    "\n",
    "    model = create_model(model_size, input_shape, num_classes)\n",
    "\n",
    "    model.compile(optimizer=optimizer,\n",
    "                  loss='sparse_categorical_crossentropy',\n",
    "                  metrics=['accuracy'])\n",
    "\n",
    "    log_dir = f\"results/{dataset_name}/{model_size}/{type(optimizer).__name__}/\" + datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
    "    os.makedirs(log_dir, exist_ok=True)\n",
    "\n",
    "    callbacks = [tf.keras.callbacks.TensorBoard(log_dir=log_dir)]\n",
    "\n",
    "    history = model.fit(\n",
    "        x_train, y_train,\n",
    "        validation_data=(x_test, y_test),\n",
    "        epochs=epochs,\n",
    "        batch_size=batch_size,\n",
    "        callbacks=callbacks,\n",
    "        verbose=2\n",
    "    )\n",
    "\n",
    "    return history\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f6295efc",
   "metadata": {},
   "outputs": [],
   "source": [
    "datasets = ['mnist', \n",
    "# 'fashion_mnist', 'cifar10'\n",
    "]\n",
    "model_sizes = ['small', 'medium', 'large']\n",
    "optimizers = {\n",
    "    'SGD': tf.keras.optimizers.SGD(),\n",
    "    'Adam': tf.keras.optimizers.Adam(),\n",
    "    'RMSprop': tf.keras.optimizers.RMSprop()\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "22cf9f24",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Training small model on mnist with SGD ---\n",
      "Epoch 1/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/ml-macos-m2/lib/python3.10/site-packages/tensorflow/python/data/ops/structured_function.py:258: UserWarning: Even though the `tf.config.experimental_run_functions_eagerly` option is set, this option does not apply to tf.data functions. To force eager execution of tf.data functions, please use `tf.data.experimental.enable_debug_mode()`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "938/938 - 13s - 14ms/step - accuracy: 0.7749 - loss: 0.9092 - val_accuracy: 0.8823 - val_loss: 0.4626\n",
      "Epoch 2/5\n",
      "938/938 - 13s - 14ms/step - accuracy: 0.8874 - loss: 0.4205 - val_accuracy: 0.9024 - val_loss: 0.3568\n",
      "Epoch 3/5\n",
      "938/938 - 12s - 13ms/step - accuracy: 0.9018 - loss: 0.3525 - val_accuracy: 0.9107 - val_loss: 0.3184\n",
      "Epoch 4/5\n",
      "938/938 - 13s - 14ms/step - accuracy: 0.9099 - loss: 0.3186 - val_accuracy: 0.9165 - val_loss: 0.2933\n",
      "Epoch 5/5\n",
      "938/938 - 13s - 14ms/step - accuracy: 0.9161 - loss: 0.2965 - val_accuracy: 0.9217 - val_loss: 0.2752\n",
      "\n",
      "--- Training small model on mnist with Adam ---\n",
      "Epoch 1/5\n",
      "938/938 - 23s - 24ms/step - accuracy: 0.9035 - loss: 0.3446 - val_accuracy: 0.9426 - val_loss: 0.1967\n",
      "Epoch 2/5\n",
      "938/938 - 23s - 24ms/step - accuracy: 0.9532 - loss: 0.1633 - val_accuracy: 0.9601 - val_loss: 0.1355\n",
      "Epoch 3/5\n",
      "938/938 - 23s - 24ms/step - accuracy: 0.9653 - loss: 0.1202 - val_accuracy: 0.9679 - val_loss: 0.1112\n",
      "Epoch 4/5\n",
      "938/938 - 23s - 25ms/step - accuracy: 0.9721 - loss: 0.0953 - val_accuracy: 0.9697 - val_loss: 0.0990\n",
      "Epoch 5/5\n",
      "938/938 - 23s - 24ms/step - accuracy: 0.9768 - loss: 0.0789 - val_accuracy: 0.9732 - val_loss: 0.0879\n",
      "\n",
      "--- Training small model on mnist with RMSprop ---\n",
      "Epoch 1/5\n",
      "938/938 - 16s - 17ms/step - accuracy: 0.9071 - loss: 0.3401 - val_accuracy: 0.9369 - val_loss: 0.2069\n",
      "Epoch 2/5\n",
      "938/938 - 16s - 17ms/step - accuracy: 0.9514 - loss: 0.1686 - val_accuracy: 0.9581 - val_loss: 0.1422\n",
      "Epoch 3/5\n",
      "938/938 - 16s - 17ms/step - accuracy: 0.9637 - loss: 0.1245 - val_accuracy: 0.9611 - val_loss: 0.1250\n",
      "Epoch 4/5\n",
      "938/938 - 17s - 18ms/step - accuracy: 0.9711 - loss: 0.1001 - val_accuracy: 0.9687 - val_loss: 0.1020\n",
      "Epoch 5/5\n",
      "938/938 - 16s - 17ms/step - accuracy: 0.9753 - loss: 0.0843 - val_accuracy: 0.9735 - val_loss: 0.0917\n",
      "\n",
      "--- Training medium model on mnist with SGD ---\n",
      "Epoch 1/5\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Unknown variable: <Variable path=sequential_3/conv2d/kernel, shape=(3, 3, 1, 32), dtype=float32, value=[[[[-0.0365007   0.06048135 -0.07531459  0.10370916 -0.03047556\n    -0.05668736  0.03010792 -0.00141899 -0.04018178  0.06333803\n     0.09014285  0.07875232 -0.10099072 -0.05712081  0.10191517\n    -0.04997095 -0.03953873 -0.01535632 -0.09141612  0.08703452\n     0.06845456  0.00731577 -0.13980752 -0.05907621  0.08032005\n     0.13709901  0.05532143 -0.00927801  0.07934175  0.02873382\n     0.02481079 -0.05120631]]\n\n  [[ 0.09291896  0.08649303 -0.04074316 -0.02634504  0.06421356\n     0.12940721  0.00873372 -0.01456688  0.11392339  0.09660241\n    -0.05467994 -0.13137269 -0.06201024 -0.00159287 -0.12166966\n    -0.01512639 -0.05282332  0.04221871  0.06175859  0.01525134\n     0.01895292  0.09232108 -0.01249762 -0.03818327  0.13394\n     0.01080692 -0.09308334  0.01048559  0.12741838  0.09904006\n     0.04582287 -0.03895862]]\n\n  [[ 0.13946585  0.03910677 -0.05318601  0.07292618 -0.10659006\n    -0.12931542  0.08710434  0.12538274  0.14114581 -0.04693397\n    -0.13414535  0.02772212  0.10298681 -0.06816351  0.07025009\n     0.1260746   0.08001469  0.11125375 -0.02608828  0.1396107\n     0.00357078  0.01626024 -0.0419817   0.1405565   0.03836386\n    -0.10866959 -0.10035387 -0.07813276  0.03364372 -0.02012745\n    -0.04511588  0.0748855 ]]]\n\n\n [[[-0.14016123  0.10638186  0.02948447  0.01747715  0.06348343\n    -0.11677802 -0.09612179 -0.11994577 -0.11724668 -0.09495579\n     0.01101701 -0.07292092  0.0358789  -0.08887976 -0.02549186\n    -0.11131738 -0.02702641 -0.00101821  0.03792478 -0.07629908\n    -0.0204911   0.07556634 -0.04486237 -0.09463128  0.06699964\n    -0.06515035  0.0253907   0.07717456  0.09661847 -0.12815166\n    -0.10718116 -0.10198037]]\n\n  [[ 0.00343317 -0.10098533 -0.02115221  0.01811533  0.00350368\n     0.01815014 -0.13917243  0.00329325  0.06890252  0.09737836\n    -0.00254345  0.03471664 -0.04743788 -0.11546031 -0.00090601\n     0.12170793  0.13601501 -0.1383567   0.1350741  -0.1270166\n     0.12081595  0.07223494 -0.08998577 -0.0290405   0.09675342\n    -0.11455603 -0.12504067  0.111237   -0.12740462 -0.13104273\n    -0.04000282  0.11229102]]\n\n  [[ 0.10394953 -0.00692518  0.03977486 -0.10951854  0.03724456\n    -0.08706939 -0.02455226  0.10682812 -0.10034259 -0.13823654\n     0.12680934 -0.06394278  0.07625768 -0.08415132 -0.10647617\n    -0.07055053 -0.1402529  -0.1330841   0.10917391 -0.12814495\n    -0.09701509 -0.0204363  -0.00040621 -0.10446695 -0.06368624\n     0.11621825  0.13233422 -0.03603976 -0.08563142  0.04037911\n     0.05221632  0.00189762]]]\n\n\n [[[ 0.0842912   0.12093179 -0.05599216 -0.12439288 -0.04704648\n     0.0882809  -0.09302069 -0.12376064 -0.0644763  -0.04908863\n     0.00375658  0.04916847 -0.04128387 -0.1406442  -0.02328955\n    -0.07000051 -0.02965058  0.14122944 -0.06264848 -0.02334123\n     0.04940762 -0.12988926 -0.06837405 -0.07479462 -0.11974157\n     0.07993235  0.09453593 -0.05633543  0.08967848 -0.06954814\n    -0.10618822 -0.03571987]]\n\n  [[-0.05307053  0.02361514 -0.06699911  0.04019293  0.02828597\n     0.04502682  0.00246765 -0.04661048  0.12103494 -0.13784818\n     0.04627357  0.07132663 -0.14192829 -0.06052811 -0.03641836\n     0.033815    0.03533253 -0.1180431   0.01522003 -0.09068704\n     0.02514271  0.13442887 -0.09614097  0.10363698 -0.05780794\n     0.00116789  0.02642125  0.07582401  0.08124721  0.02052763\n    -0.0468666   0.04167472]]\n\n  [[ 0.0983697   0.05026636 -0.0234452   0.07169898  0.11658277\n     0.008578   -0.07501841 -0.02274552  0.08629097 -0.0601477\n     0.10916607 -0.11128302 -0.00936538  0.10984631 -0.0867821\n    -0.07505108  0.08355117 -0.13511677  0.03705859  0.03168108\n     0.10961105  0.00837871  0.08665125 -0.07688557  0.08686048\n    -0.10510501 -0.07239729 -0.0940551  -0.10961169 -0.01869382\n     0.07587084 -0.09531008]]]]>. This optimizer can only be called for the variables it was originally built with. When working with a new set of variables, you should recreate a new optimizer instance.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[9], line 5\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m name, opt \u001b[38;5;129;01min\u001b[39;00m optimizers\u001b[38;5;241m.\u001b[39mitems():\n\u001b[1;32m      4\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m--- Training \u001b[39m\u001b[38;5;132;01m{\u001b[39;00msize\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m model on \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mdataset\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m with \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mname\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m ---\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m----> 5\u001b[0m     \u001b[43mtrain_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msize\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdataset_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdataset\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mopt\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[6], line 17\u001b[0m, in \u001b[0;36mtrain_model\u001b[0;34m(model_size, dataset_name, optimizer, epochs, batch_size)\u001b[0m\n\u001b[1;32m     13\u001b[0m os\u001b[38;5;241m.\u001b[39mmakedirs(log_dir, exist_ok\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m     15\u001b[0m callbacks \u001b[38;5;241m=\u001b[39m [tf\u001b[38;5;241m.\u001b[39mkeras\u001b[38;5;241m.\u001b[39mcallbacks\u001b[38;5;241m.\u001b[39mTensorBoard(log_dir\u001b[38;5;241m=\u001b[39mlog_dir)]\n\u001b[0;32m---> 17\u001b[0m history \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     18\u001b[0m \u001b[43m    \u001b[49m\u001b[43mx_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_train\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     19\u001b[0m \u001b[43m    \u001b[49m\u001b[43mvalidation_data\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mx_test\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_test\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     20\u001b[0m \u001b[43m    \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mepochs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     21\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbatch_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     22\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcallbacks\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     23\u001b[0m \u001b[43m    \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m2\u001b[39;49m\n\u001b[1;32m     24\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     26\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m history\n",
      "File \u001b[0;32m/opt/anaconda3/envs/ml-macos-m2/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py:122\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    119\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n\u001b[1;32m    120\u001b[0m     \u001b[38;5;66;03m# To get the full stack trace, call:\u001b[39;00m\n\u001b[1;32m    121\u001b[0m     \u001b[38;5;66;03m# `keras.config.disable_traceback_filtering()`\u001b[39;00m\n\u001b[0;32m--> 122\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\u001b[38;5;241m.\u001b[39mwith_traceback(filtered_tb) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    123\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m    124\u001b[0m     \u001b[38;5;28;01mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[0;32m/opt/anaconda3/envs/ml-macos-m2/lib/python3.10/site-packages/keras/src/optimizers/base_optimizer.py:329\u001b[0m, in \u001b[0;36mBaseOptimizer._check_variables_are_known\u001b[0;34m(self, variables)\u001b[0m\n\u001b[1;32m    327\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m v \u001b[38;5;129;01min\u001b[39;00m variables:\n\u001b[1;32m    328\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_var_key(v) \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_trainable_variables_indices:\n\u001b[0;32m--> 329\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    330\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUnknown variable: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mv\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m. This optimizer can only \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    331\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbe called for the variables it was originally built with. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    332\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mWhen working with a new set of variables, you should \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    333\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrecreate a new optimizer instance.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    334\u001b[0m         )\n",
      "\u001b[0;31mValueError\u001b[0m: Unknown variable: <Variable path=sequential_3/conv2d/kernel, shape=(3, 3, 1, 32), dtype=float32, value=[[[[-0.0365007   0.06048135 -0.07531459  0.10370916 -0.03047556\n    -0.05668736  0.03010792 -0.00141899 -0.04018178  0.06333803\n     0.09014285  0.07875232 -0.10099072 -0.05712081  0.10191517\n    -0.04997095 -0.03953873 -0.01535632 -0.09141612  0.08703452\n     0.06845456  0.00731577 -0.13980752 -0.05907621  0.08032005\n     0.13709901  0.05532143 -0.00927801  0.07934175  0.02873382\n     0.02481079 -0.05120631]]\n\n  [[ 0.09291896  0.08649303 -0.04074316 -0.02634504  0.06421356\n     0.12940721  0.00873372 -0.01456688  0.11392339  0.09660241\n    -0.05467994 -0.13137269 -0.06201024 -0.00159287 -0.12166966\n    -0.01512639 -0.05282332  0.04221871  0.06175859  0.01525134\n     0.01895292  0.09232108 -0.01249762 -0.03818327  0.13394\n     0.01080692 -0.09308334  0.01048559  0.12741838  0.09904006\n     0.04582287 -0.03895862]]\n\n  [[ 0.13946585  0.03910677 -0.05318601  0.07292618 -0.10659006\n    -0.12931542  0.08710434  0.12538274  0.14114581 -0.04693397\n    -0.13414535  0.02772212  0.10298681 -0.06816351  0.07025009\n     0.1260746   0.08001469  0.11125375 -0.02608828  0.1396107\n     0.00357078  0.01626024 -0.0419817   0.1405565   0.03836386\n    -0.10866959 -0.10035387 -0.07813276  0.03364372 -0.02012745\n    -0.04511588  0.0748855 ]]]\n\n\n [[[-0.14016123  0.10638186  0.02948447  0.01747715  0.06348343\n    -0.11677802 -0.09612179 -0.11994577 -0.11724668 -0.09495579\n     0.01101701 -0.07292092  0.0358789  -0.08887976 -0.02549186\n    -0.11131738 -0.02702641 -0.00101821  0.03792478 -0.07629908\n    -0.0204911   0.07556634 -0.04486237 -0.09463128  0.06699964\n    -0.06515035  0.0253907   0.07717456  0.09661847 -0.12815166\n    -0.10718116 -0.10198037]]\n\n  [[ 0.00343317 -0.10098533 -0.02115221  0.01811533  0.00350368\n     0.01815014 -0.13917243  0.00329325  0.06890252  0.09737836\n    -0.00254345  0.03471664 -0.04743788 -0.11546031 -0.00090601\n     0.12170793  0.13601501 -0.1383567   0.1350741  -0.1270166\n     0.12081595  0.07223494 -0.08998577 -0.0290405   0.09675342\n    -0.11455603 -0.12504067  0.111237   -0.12740462 -0.13104273\n    -0.04000282  0.11229102]]\n\n  [[ 0.10394953 -0.00692518  0.03977486 -0.10951854  0.03724456\n    -0.08706939 -0.02455226  0.10682812 -0.10034259 -0.13823654\n     0.12680934 -0.06394278  0.07625768 -0.08415132 -0.10647617\n    -0.07055053 -0.1402529  -0.1330841   0.10917391 -0.12814495\n    -0.09701509 -0.0204363  -0.00040621 -0.10446695 -0.06368624\n     0.11621825  0.13233422 -0.03603976 -0.08563142  0.04037911\n     0.05221632  0.00189762]]]\n\n\n [[[ 0.0842912   0.12093179 -0.05599216 -0.12439288 -0.04704648\n     0.0882809  -0.09302069 -0.12376064 -0.0644763  -0.04908863\n     0.00375658  0.04916847 -0.04128387 -0.1406442  -0.02328955\n    -0.07000051 -0.02965058  0.14122944 -0.06264848 -0.02334123\n     0.04940762 -0.12988926 -0.06837405 -0.07479462 -0.11974157\n     0.07993235  0.09453593 -0.05633543  0.08967848 -0.06954814\n    -0.10618822 -0.03571987]]\n\n  [[-0.05307053  0.02361514 -0.06699911  0.04019293  0.02828597\n     0.04502682  0.00246765 -0.04661048  0.12103494 -0.13784818\n     0.04627357  0.07132663 -0.14192829 -0.06052811 -0.03641836\n     0.033815    0.03533253 -0.1180431   0.01522003 -0.09068704\n     0.02514271  0.13442887 -0.09614097  0.10363698 -0.05780794\n     0.00116789  0.02642125  0.07582401  0.08124721  0.02052763\n    -0.0468666   0.04167472]]\n\n  [[ 0.0983697   0.05026636 -0.0234452   0.07169898  0.11658277\n     0.008578   -0.07501841 -0.02274552  0.08629097 -0.0601477\n     0.10916607 -0.11128302 -0.00936538  0.10984631 -0.0867821\n    -0.07505108  0.08355117 -0.13511677  0.03705859  0.03168108\n     0.10961105  0.00837871  0.08665125 -0.07688557  0.08686048\n    -0.10510501 -0.07239729 -0.0940551  -0.10961169 -0.01869382\n     0.07587084 -0.09531008]]]]>. This optimizer can only be called for the variables it was originally built with. When working with a new set of variables, you should recreate a new optimizer instance."
     ]
    }
   ],
   "source": [
    "for dataset in datasets:\n",
    "    for size in model_sizes:\n",
    "        for name, opt in optimizers.items():\n",
    "            print(f\"\\n--- Training {size} model on {dataset} with {name} ---\")\n",
    "            train_model(model_size=size, dataset_name=dataset, optimizer=opt)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c0ac770",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml-macos-m2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
